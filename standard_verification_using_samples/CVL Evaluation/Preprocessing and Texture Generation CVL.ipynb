{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxFo6-vB-6i5"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "import gdown\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "junVADgmbo2H"
   },
   "source": [
    "# Creating Textures - CVL Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCjbJ1CqPmwS"
   },
   "source": [
    "## Extracting Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8042,
     "status": "ok",
     "timestamp": 1753244219465,
     "user": {
      "displayName": "MDR Sandamini",
      "userId": "14130589177191734805"
     },
     "user_tz": -330
    },
    "id": "XLp_n6X4QBgT",
    "outputId": "414d3cd1-e887-46fa-d3f8-850703951cad"
   },
   "outputs": [],
   "source": [
    "file_id = \"file_id\"\n",
    "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", \"binary_cvl.zip\", quiet=False)\n",
    "\n",
    "with zipfile.ZipFile(\"binary_cvl.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"cvl_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mlern8QZPz6o"
   },
   "source": [
    "## Extracting Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bThO160AQPdl"
   },
   "outputs": [],
   "source": [
    "def segment_words(lines):\n",
    "    cropped_words = []\n",
    "    for line in lines:\n",
    "        vertical_projection = np.sum(line, axis=0)\n",
    "        word_threshold = 44000\n",
    "        word_start = None\n",
    "        words = []\n",
    "        consecutive_count = 0\n",
    "        min_consecutive = 20\n",
    "        for i, value in enumerate(vertical_projection):\n",
    "            if value < word_threshold:\n",
    "                consecutive_count = 0\n",
    "                if word_start is None:\n",
    "                    word_start = i\n",
    "            elif value >= word_threshold:\n",
    "                if word_start is not None:\n",
    "                    consecutive_count += 1\n",
    "                    if consecutive_count >= min_consecutive:\n",
    "                        words.append((word_start, i - min_consecutive + 1))\n",
    "                        word_start = None\n",
    "                        consecutive_count = 0\n",
    "        for word in words:\n",
    "            start, end = word\n",
    "            cropped_words.append(line[:, start:end])\n",
    "\n",
    "    return cropped_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfsVP8oVQY0R"
   },
   "source": [
    "## Generating Texture - Line Filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pP_x_Rl0QbHd"
   },
   "outputs": [],
   "source": [
    "def generate_texture(cropped_words, file_name=\"unknown\"):\n",
    "    canvas_height = 950\n",
    "    canvas_width = 950\n",
    "    if not cropped_words:\n",
    "        print(f\"[WARNING] No cropped words for file: {file_name}\")\n",
    "        return np.ones((canvas_height, canvas_width), dtype=np.uint8) * 255\n",
    "\n",
    "    row_spacing = -100\n",
    "    column_spacing = 0\n",
    "\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    current_width = 0\n",
    "\n",
    "    for word in cropped_words:\n",
    "        word_height, word_width = word.shape\n",
    "\n",
    "        while word_width > 0:\n",
    "            remaining_space = canvas_width - current_width\n",
    "\n",
    "            if remaining_space >= word_width:\n",
    "                current_row.append(word)\n",
    "                current_width += word_width + column_spacing\n",
    "                break\n",
    "            else:\n",
    "                if remaining_space > 0:\n",
    "                    word_part = word[:, :remaining_space]\n",
    "                    current_row.append(word_part)\n",
    "                    word = word[:, remaining_space:]\n",
    "                    word_width = word.shape[1]\n",
    "                rows.append(current_row)\n",
    "                current_row = []\n",
    "                current_width = 0\n",
    "\n",
    "    if not rows:\n",
    "        print(f\"[WARNING] No rows could be constructed from the cropped words in file: {file_name}\")\n",
    "        return np.ones((canvas_height, canvas_width), dtype=np.uint8) * 255\n",
    "\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "\n",
    "\n",
    "    used_width = sum(w.shape[1] for w in rows[-1]) + column_spacing * (len(rows[-1]) - 1)\n",
    "    remaining_space = canvas_width - used_width\n",
    "    for w in rows[0]:\n",
    "      if remaining_space <= 0:\n",
    "          break\n",
    "      h, w_w = w.shape\n",
    "      if w_w + column_spacing <= remaining_space:\n",
    "          rows[-1].append(w)\n",
    "          remaining_space -= (w_w + column_spacing)\n",
    "      else:\n",
    "          slice_width = remaining_space\n",
    "          rows[-1].append(w[:, :slice_width])\n",
    "          remaining_space = 0\n",
    "\n",
    "\n",
    "    texture = np.ones((canvas_height, canvas_width), dtype=np.uint8) * 255\n",
    "\n",
    "    y_offset = 0\n",
    "    row_index = 0\n",
    "\n",
    "    while y_offset < canvas_height:\n",
    "        row = rows[row_index % len(rows)]  \n",
    "        x_offset = 0\n",
    "        max_row_height = max(word.shape[0] for word in row)\n",
    "\n",
    "        if y_offset + max_row_height > canvas_height:\n",
    "            break\n",
    "\n",
    "        for word in row:\n",
    "            word_height, word_width = word.shape\n",
    "            if y_offset + word_height <= canvas_height and x_offset + word_width <= canvas_width:\n",
    "                word_text = np.ones((canvas_height, canvas_width), dtype=np.uint8) * 255\n",
    "                word_text[y_offset:y_offset + word_height, x_offset:x_offset + word_width] = word\n",
    "                texture = cv2.bitwise_not(cv2.bitwise_or(cv2.bitwise_not(texture), cv2.bitwise_not(word_text)))\n",
    "            x_offset += word_width + column_spacing\n",
    "\n",
    "        y_offset += max_row_height + row_spacing\n",
    "        row_index += 1\n",
    "\n",
    "    crop_size = 900\n",
    "\n",
    "    h, w = texture.shape\n",
    "    y_start = (h - crop_size) // 2\n",
    "    x_start = (w - crop_size) // 2\n",
    "    y_end   = y_start + crop_size\n",
    "    x_end   = x_start + crop_size\n",
    "\n",
    "    texture = texture[\n",
    "        y_start : y_end,\n",
    "        x_start : x_end\n",
    "    ]\n",
    "\n",
    "    return texture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3p7nLSRQqVJ"
   },
   "outputs": [],
   "source": [
    "def split_texture(texture):\n",
    "\n",
    "    patch_size = 450\n",
    "    patches = []\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            y_start = i * patch_size\n",
    "            x_start = j * patch_size\n",
    "            patch = texture[y_start:y_start + patch_size, x_start:x_start + patch_size]\n",
    "            patches.append(patch)\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jiSvzbAQxd7"
   },
   "source": [
    "## Creating Textures for all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 173696,
     "status": "ok",
     "timestamp": 1753244400696,
     "user": {
      "displayName": "MDR Sandamini",
      "userId": "14130589177191734805"
     },
     "user_tz": -330
    },
    "id": "bj2SA4Q7RLir",
    "outputId": "61fada6c-07c4-472b-f338-cc7f1a65e20f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 149/309 [01:25<01:24,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No rows could be constructed from the cropped words in file: cvl_dataset/data/binary_cvl/CVL0286/CVL0286_6.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 194/309 [01:51<00:54,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] No cropped words for file: cvl_dataset/data/binary_cvl/CVL0431/CVL0431_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 309/309 [02:53<00:00,  1.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "input_root = 'cvl_dataset/data/binary_cvl'\n",
    "output_root = 'texture_cvl'\n",
    "\n",
    "for writer_id in tqdm(sorted(os.listdir(input_root))):\n",
    "    writer_path = os.path.join(input_root, writer_id)\n",
    "    if not os.path.isdir(writer_path):\n",
    "        continue\n",
    "\n",
    "    for filename in sorted(os.listdir(writer_path)):\n",
    "        if not filename.endswith(\".png\"):\n",
    "            continue\n",
    "\n",
    "        image_path = os.path.join(writer_path, filename)\n",
    "\n",
    "        try:\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            cleaned_lines, _ = segment_lines(image)\n",
    "\n",
    "            cropped_words = segment_words(cleaned_lines)\n",
    "\n",
    "            texture = generate_texture(cropped_words,file_name = image_path)\n",
    "\n",
    "            patches = split_texture(texture)\n",
    "\n",
    "            parts = filename.split(\".\")\n",
    "            base_id = parts[0]\n",
    "            writer_id = parts[0].split(\"_\")[0]\n",
    "            # sample_id = parts[1]\n",
    "            # speed = parts[2].split(\".\")[0]\n",
    "\n",
    "            save_dir = os.path.join(output_root,writer_id,base_id)\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            for i, patch in enumerate(patches):\n",
    "                patch_filename = f\"{base_id}_T{i+1}.png\"\n",
    "                patch_path = os.path.join(save_dir, patch_filename)\n",
    "                cv2.imwrite(patch_path, patch)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed processing: {image_path}\")\n",
    "            print(f\"        {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnEYxsxeSu8T"
   },
   "source": [
    "## Upload Textures to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 31476,
     "status": "ok",
     "timestamp": 1753244434093,
     "user": {
      "displayName": "MDR Sandamini",
      "userId": "14130589177191734805"
     },
     "user_tz": -330
    },
    "id": "AqN5voHlSzB2",
    "outputId": "f4b30798-262e-4083-ce7e-174b58fe5053"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"texture_cvl\", 'zip', \"texture_cvl\")\n",
    "\n",
    "from google.colab import files\n",
    "files.download(\"texture_cvl.zip\")\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "target_directory = \"/content/drive/MyDrive/PATH\"\n",
    "!cp texture_cvl.zip \"{target_directory}/\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "TdPEccosdlhF",
    "EHBPi56-lEB-",
    "_liSm1k9bxOV",
    "wCjbJ1CqPmwS",
    "Mlern8QZPz6o",
    "RfsVP8oVQY0R",
    "9jiSvzbAQxd7",
    "UnEYxsxeSu8T"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
