{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9368,
     "status": "ok",
     "timestamp": 1749793846692,
     "user": {
      "displayName": "Rashmi Sandamini",
      "userId": "17082450369188033534"
     },
     "user_tz": -330
    },
    "id": "wiGAdK-H1uiu",
    "outputId": "1afc4441-ccab-4641-fefd-decc35aee143"
   },
   "outputs": [],
   "source": [
    "!pip install keras_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20299,
     "status": "ok",
     "timestamp": 1749793885538,
     "user": {
      "displayName": "Rashmi Sandamini",
      "userId": "17082450369188033534"
     },
     "user_tz": -330
    },
    "id": "gPCFqzOBAULj",
    "outputId": "42645c21-3e58-4496-de1b-4ebd61b3a026"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras_cv.models import ResNetBackbone\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_curve, precision_recall_curve, auc, precision_recall_fscore_support, roc_auc_score\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import gdown\n",
    "import math\n",
    "import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from keras.saving import register_keras_serializable\n",
    "import gdown\n",
    "import zipfile\n",
    "print(tf.__version__)\n",
    "!python --version\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "DATASET_DIR = 'Data_V5_Res'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQqarNltWSXo"
   },
   "source": [
    "# Dataset Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79414,
     "status": "ok",
     "timestamp": 1749793996728,
     "user": {
      "displayName": "Rashmi Sandamini",
      "userId": "17082450369188033534"
     },
     "user_tz": -330
    },
    "id": "qCe05E2_2W3Z",
    "outputId": "8662e539-02cc-4b76-92a2-2030c583dd4c"
   },
   "outputs": [],
   "source": [
    "file_id = \"google_drive_file_id\"\n",
    "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", \"Data_V5_Res.zip\", quiet=False)\n",
    "with zipfile.ZipFile(\"Data_V5_Res.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1749794145522,
     "user": {
      "displayName": "Rashmi Sandamini",
      "userId": "17082450369188033534"
     },
     "user_tz": -330
    },
    "id": "JtvBeYchWUdo"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "def load_image(rel_path):\n",
    "    try:\n",
    "        img = np.load(DATASET_DIR + \"/\" + rel_path + \".npy\")\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load .npy file: {rel_path}\\n{e}\")\n",
    "\n",
    "def make_generator(df):\n",
    "    def generator():\n",
    "        for _, row in df.iterrows():\n",
    "            img1 = load_image(row[\"sample_1\"])\n",
    "            img2 = load_image(row[\"sample_2\"])\n",
    "            label = np.array(row[\"label\"], dtype=np.float32)\n",
    "            yield (img1, img2), label\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "executionInfo": {
     "elapsed": 1522,
     "status": "ok",
     "timestamp": 1749794150500,
     "user": {
      "displayName": "Rashmi Sandamini",
      "userId": "17082450369188033534"
     },
     "user_tz": -330
    },
    "id": "hwvquYz-BUHb",
    "outputId": "f4932475-9f26-4a86-c50e-67a42ccf0067"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(f'{DATASET_DIR}/train.parquet')\n",
    "val_df = pd.read_parquet(f'{DATASET_DIR}/val.parquet')\n",
    "sample_path = train_df.iloc[0][\"sample_1\"]\n",
    "print(sample_path)\n",
    "img = load_image(sample_path)\n",
    "\n",
    "print(\"Image shape:\", img.shape)\n",
    "print(\"Pixel value range:\", np.min(img), \"to\", np.max(img))\n",
    "plt.imshow(img)\n",
    "plt.title(\"Preprocessed Image\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXYUF_roWadJ"
   },
   "source": [
    "# Siamese Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1749794263612,
     "user": {
      "displayName": "Rashmi Sandamini",
      "userId": "17082450369188033534"
     },
     "user_tz": -330
    },
    "id": "Xvs0Xbb0CFNz"
   },
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "def l2_normalize(x):\n",
    "    return tf.math.l2_normalize(x, axis=1)\n",
    "\n",
    "@register_keras_serializable()\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = tf.reduce_sum(tf.square(x - y), axis=1, keepdims=True)\n",
    "    return tf.sqrt(tf.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "@register_keras_serializable()\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    square_pred = tf.square(y_pred)\n",
    "    margin = 0.338\n",
    "    margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
    "    return tf.reduce_mean((1 - y_true) * square_pred + y_true * margin_square)\n",
    "\n",
    "@register_keras_serializable()\n",
    "def output_shape(input_shapes):\n",
    "    return (input_shapes[0][0], 1)\n",
    "\n",
    "output_signature = (\n",
    "    (\n",
    "        tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32)\n",
    "    ),\n",
    "    tf.TensorSpec(shape=(), dtype=tf.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "executionInfo": {
     "elapsed": 5896,
     "status": "ok",
     "timestamp": 1749794274067,
     "user": {
      "displayName": "Rashmi Sandamini",
      "userId": "17082450369188033534"
     },
     "user_tz": -330
    },
    "id": "47V1dqV1WcFM",
    "outputId": "72ee46cf-d20e-4341-87b4-3fe02acf581e"
   },
   "outputs": [],
   "source": [
    "backbone = ResNetBackbone.from_preset(\n",
    "    \"resnet18\",\n",
    "    load_weights=\"imagenet\",\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "proj_inp = Input(shape=(224, 224, 3), name=\"proj_input\")\n",
    "feat_map = backbone(proj_inp)\n",
    "x   = GlobalAveragePooling2D()(feat_map)\n",
    "x   = Dense(512, activation=\"relu\", name=\"dense_1\")(x)\n",
    "x   = Dense(256, activation=\"relu\", name=\"dense_2\")(x)\n",
    "emb = Lambda(l2_normalize,name=\"head_l2_norm\")(x)\n",
    "feature_extractor = Model(proj_inp, emb, name=\"resnet_backbone\")\n",
    "input_a = Input(shape=(224, 224, 3), name='input_a')\n",
    "input_b = Input(shape=(224, 224, 3), name='input_b')\n",
    "feat_a = feature_extractor(input_a)\n",
    "feat_b = feature_extractor(input_b)\n",
    "output_layer = Lambda(euclidean_distance, output_shape=output_shape, name='euclidean_distance')([feat_a, feat_b])\n",
    "siamese_net = Model(inputs=[input_a, input_b], outputs=output_layer, name='siamese_net')\n",
    "siamese_net.compile(\n",
    "    loss=contrastive_loss,\n",
    "    optimizer=keras.optimizers.Adam(1e-4),\n",
    "    metrics=[AUC(name=\"roc_auc\")]\n",
    "    )\n",
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0NVdm2LA1U5"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGKxV_KYK3Vj"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: make_generator(train_df)(),\n",
    "    output_signature=output_signature\n",
    ").batch(BATCH_SIZE).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: make_generator(val_df)(),\n",
    "    output_signature=output_signature\n",
    ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='best_siamese_model.keras',\n",
    "    monitor='val_roc_auc',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_roc_auc',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "class DistanceHistCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, val_ds, batches_to_use=100, log_dir=\"histograms\"):\n",
    "        super().__init__()\n",
    "        self.val_ds = val_ds\n",
    "        self.batches_to_use = batches_to_use\n",
    "        self.log_dir = pathlib.Path(log_dir)\n",
    "        self.log_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        dists, labels = [], []\n",
    "\n",
    "        for i, ((img1, img2), y) in enumerate(self.val_ds):\n",
    "            if i >= self.batches_to_use:\n",
    "                break\n",
    "            pred = self.model.predict([img1, img2],verbose=0).ravel()\n",
    "            dists.append(pred)\n",
    "            labels.append(y.numpy())\n",
    "\n",
    "        dists  = np.concatenate(dists)\n",
    "        labels = np.concatenate(labels)\n",
    "\n",
    "        neg = dists[labels == 0]\n",
    "        pos = dists[labels == 1]\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.hist(neg, bins=50, alpha=0.6, label=\"Same Writer\")\n",
    "        plt.hist(pos, bins=50, alpha=0.6, label=\"Different Writer\")\n",
    "        plt.title(f\"Distance distribution - epoch {epoch + 1}\")\n",
    "        plt.xlabel(\"Euclidean distance\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        fname = self.log_dir / f\"epoch_{epoch + 1:03d}.png\"\n",
    "        plt.savefig(fname, dpi=120)\n",
    "        plt.close()\n",
    "\n",
    "histogram_callback = DistanceHistCallback(\n",
    "    val_ds          = val_dataset,\n",
    "    batches_to_use  = 50\n",
    ")\n",
    "\n",
    "!rm -rf histograms\n",
    "!mkdir histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "executionInfo": {
     "elapsed": 3415553,
     "status": "error",
     "timestamp": 1747989939479,
     "user": {
      "displayName": "Hikari",
      "userId": "13771564110195447892"
     },
     "user_tz": -330
    },
    "id": "Rpc84qnaWk8I",
    "outputId": "73462576-a0be-4617-cc71-50aa871d03a8"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS_STAGE1 = 3\n",
    "NUM_EPOCHS_STAGE2 = 5\n",
    "NUM_EPOCHS_STAGE3 = 40\n",
    "\n",
    "MODEL_DIR = 'siamese_checkpoints'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "def make_train_ds():\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: make_generator(train_df)(),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# ─── Stage 1: head only (epochs 0,1,2) ────────────────────────────────\n",
    "for epoch in range(NUM_EPOCHS_STAGE1):\n",
    "    train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    train_ds = make_train_ds()\n",
    "\n",
    "    for l in backbone.layers:\n",
    "        l.trainable = False\n",
    "\n",
    "    siamese_net.compile(\n",
    "        loss=contrastive_loss,\n",
    "        optimizer=keras.optimizers.Adam(1e-4),\n",
    "        metrics=[AUC(name=\"roc_auc\")]\n",
    "    )\n",
    "\n",
    "    siamese_net.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=epoch+1,\n",
    "        initial_epoch=epoch,\n",
    "        steps_per_epoch   = math.ceil(len(train_df) / BATCH_SIZE),\n",
    "        validation_steps  = math.ceil(len(val_df)   / BATCH_SIZE),\n",
    "        callbacks=[checkpoint_callback,\n",
    "                   early_stopping_callback,\n",
    "                   histogram_callback]\n",
    "    )\n",
    "\n",
    "    fname = os.path.join(MODEL_DIR, f\"epoch_{epoch+1:02d}.keras\")\n",
    "    siamese_net.save(fname)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_STAGE1, NUM_EPOCHS_STAGE2):\n",
    "    train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "    train_ds = make_train_ds()\n",
    "\n",
    "    for l in backbone.layers:\n",
    "        l.trainable = l.name.startswith(\"v2_stack_3_\")\n",
    "\n",
    "    siamese_net.compile(\n",
    "        loss=contrastive_loss,\n",
    "        optimizer=keras.optimizers.Adam(1e-5),\n",
    "        metrics=[AUC(name=\"roc_auc\")]\n",
    "    )\n",
    "\n",
    "    siamese_net.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=epoch+1,\n",
    "        initial_epoch=epoch,\n",
    "        steps_per_epoch   = math.ceil(len(train_df) / BATCH_SIZE),\n",
    "        validation_steps  = math.ceil(len(val_df)   / BATCH_SIZE),\n",
    "        callbacks=[checkpoint_callback,\n",
    "                   early_stopping_callback,\n",
    "                   histogram_callback]\n",
    "    )\n",
    "\n",
    "    fname = os.path.join(MODEL_DIR, f\"epoch_{epoch+1:02d}.keras\")\n",
    "    siamese_net.save(fname)\n",
    "\n",
    "# ─── Stage 3: full finetune (epochs 8–39) ───────────────────────────────\n",
    "for epoch in range(NUM_EPOCHS_STAGE2, NUM_EPOCHS_STAGE3):\n",
    "    train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "    train_ds = make_train_ds()\n",
    "\n",
    "    for l in backbone.layers:\n",
    "        l.trainable = True\n",
    "\n",
    "    siamese_net.compile(\n",
    "        loss=contrastive_loss,\n",
    "        optimizer=keras.optimizers.Adam(1e-6),\n",
    "        metrics=[AUC(name=\"roc_auc\")]\n",
    "    )\n",
    "\n",
    "    siamese_net.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=epoch+1,\n",
    "        initial_epoch=epoch,\n",
    "        steps_per_epoch   = math.ceil(len(train_df) / BATCH_SIZE),\n",
    "        validation_steps  = math.ceil(len(val_df)   / BATCH_SIZE),\n",
    "        callbacks=[checkpoint_callback,\n",
    "                   early_stopping_callback,\n",
    "                   histogram_callback]\n",
    "    )\n",
    "\n",
    "    fname = os.path.join(MODEL_DIR, f\"epoch_{epoch+1:02d}.keras\")\n",
    "    siamese_net.save(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mi1BehRJK3Vm"
   },
   "source": [
    "# Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "executionInfo": {
     "elapsed": 363158,
     "status": "ok",
     "timestamp": 1747990309660,
     "user": {
      "displayName": "Hikari",
      "userId": "13771564110195447892"
     },
     "user_tz": -330
    },
    "id": "R6rnOW20_S8J",
    "outputId": "9bf48eb1-e944-4fdb-e4ee-dff48d5ac918"
   },
   "outputs": [],
   "source": [
    "def plot_scatter(labels, scores, dataset=\"Test\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(range(len(scores)), scores, c=labels, cmap='coolwarm', label='Distance', alpha=0.4)\n",
    "    plt.title(f'[{dataset}] Predicted Distances Colored by Label')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.colorbar(label='True Label (0: Same, 1: Different)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_histogram(labels, scores, dataset=\"Test\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(scores[labels == 0], bins=50, alpha=0.6, label='Same Writer')\n",
    "    plt.hist(scores[labels == 1], bins=50, alpha=0.6, label='Different Writer')\n",
    "    plt.title(f'[{dataset}] Predicted Distances Histogram')\n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "model = tf.keras.models.load_model(\"best_siamese_model.keras\",safe_mode=False)\n",
    "thresholds = {'eer': 0, 'f1': 0, 'bf': 0}\n",
    "val_dist = model.predict(val_dataset).ravel()\n",
    "val_lab  = val_df[\"label\"].to_numpy().astype(int)\n",
    "plot_histogram(val_lab, val_dist, \"Validation\")\n",
    "\n",
    "fpr, tpr, thr = roc_curve(val_lab, val_dist)\n",
    "eer_idx  = np.argmin(np.abs(fpr - (1 - tpr)))\n",
    "eer_thr  = thr[eer_idx]\n",
    "thresholds['eer'] = eer_thr\n",
    "print(f\"EER threshold = {eer_thr:.3f}\")\n",
    "\n",
    "prec, rec, thr_pr = precision_recall_curve(val_lab, val_dist)\n",
    "thr_pr = np.append(thr_pr, thr_pr[-1])           \n",
    "f1 = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "f1_thr = thr_pr[np.argmax(f1)]\n",
    "thresholds['f1'] = f1_thr\n",
    "print(f\"F1-opt threshold = {f1_thr:.3f}\")\n",
    "\n",
    "best_acc = 0.0\n",
    "best_thr = 0.0\n",
    "for t in np.linspace(0.0, 1.4, 1410):\n",
    "    preds = (val_dist > t).astype(int)\n",
    "    acc   = (preds == val_lab).mean()\n",
    "    if acc > best_acc:\n",
    "        best_acc, best_thr = acc, t\n",
    "thresholds['bf'] = best_thr\n",
    "\n",
    "print(f\"Best threshold = {best_thr:.2f},  accuracy = {best_acc:.3%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9e6yylSIofs",
    "outputId": "ee6bee28-f758-4b93-b776-da2cada0f573"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: make_generator(test_df)(),\n",
    "    output_signature=output_signature\n",
    ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_df = pd.read_parquet(f'{DATASET_DIR}/test.parquet')\n",
    "\n",
    "scores = model.predict(test_dataset, verbose=0).ravel()\n",
    "labels  = test_df[\"label\"].to_numpy().astype(int)\n",
    "plot_histogram(labels, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVwTo6ICK3Vm",
    "outputId": "4b8a70bb-961f-4e90-b3f2-7521690f6a42"
   },
   "outputs": [],
   "source": [
    "for kind, threshold in thresholds.items():\n",
    "  test_pred = (scores >= threshold).astype(int)\n",
    "  acc  = accuracy_score(labels, test_pred)\n",
    "  prec, rec, f1, _ = precision_recall_fscore_support(labels, test_pred, average=\"binary\")\n",
    "  auc  = roc_auc_score(labels, scores)\n",
    "  print(f\"Threshold used     : {kind}\")\n",
    "  print(f\"Threshold          : {threshold}\")\n",
    "  print(f\"ROC-AUC            : {auc:.4f}\")\n",
    "  print(f\"Accuracy           : {acc:.4f}\")\n",
    "  print(f\"Precision          : {prec:.4f}\")\n",
    "  print(f\"Recall             : {rec:.4f}\")\n",
    "  print(f\"F1-score           : {f1:.4f}\")\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
