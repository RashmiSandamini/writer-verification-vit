{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5027,
     "status": "ok",
     "timestamp": 1753207427846,
     "user": {
      "displayName": "Hikari Research",
      "userId": "13771564110195447892"
     },
     "user_tz": -330
    },
    "id": "gPCFqzOBAULj",
    "outputId": "659b775c-118f-47ea-dc3f-971902c50f11"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gdown\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import zipfile\n",
    "print(tf.__version__)\n",
    "!python --version\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19275,
     "status": "ok",
     "timestamp": 1753207447127,
     "user": {
      "displayName": "Hikari Research",
      "userId": "13771564110195447892"
     },
     "user_tz": -330
    },
    "id": "io27Gkn1GzrG",
    "outputId": "2d92ee05-28cc-4363-ee34-48e6c5958f7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=17wriWuCNfNfSDmR94jWScO03ItmWPmnv\n",
      "From (redirected): https://drive.google.com/uc?id=17wriWuCNfNfSDmR94jWScO03ItmWPmnv&confirm=t&uuid=f6c2d543-0fe2-49eb-afd6-cb8d7116a76a\n",
      "To: /content/texture_uni.zip\n",
      "100%|██████████| 279M/279M [00:04<00:00, 66.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "file_id = \"file_id\"\n",
    "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", \"texture_uni.zip\", quiet=False)\n",
    "with zipfile.ZipFile(\"texture_uni.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pYKjAn2EJUr"
   },
   "source": [
    "## Parquet Creation Testing - Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1753199663427,
     "user": {
      "displayName": "Hikari Research",
      "userId": "13771564110195447892"
     },
     "user_tz": -330
    },
    "id": "8g8CR7-2EMod",
    "outputId": "691a10e2-54dc-4299-d8ef-32261bbb3312"
   },
   "outputs": [],
   "source": [
    "BASE_PATH = \"texture_uni\"\n",
    "random.seed(42)\n",
    "\n",
    "writer_data = {}\n",
    "\n",
    "for writer_id in os.listdir(BASE_PATH):\n",
    "    writer_path = os.path.join(BASE_PATH, writer_id)\n",
    "    if not os.path.isdir(writer_path):\n",
    "        continue\n",
    "\n",
    "    writer_data[writer_id] = {\"N\": [], \"F\": []}\n",
    "\n",
    "    for sample_folder in os.listdir(writer_path):\n",
    "        full_path = os.path.join(BASE_PATH, writer_id, sample_folder).replace(\"\\\\\", \"/\")\n",
    "        if sample_folder.endswith(\"_N\"):\n",
    "            writer_data[writer_id][\"N\"].append(full_path)\n",
    "        elif sample_folder.endswith(\"_F\"):\n",
    "            writer_data[writer_id][\"F\"].append(full_path)\n",
    "\n",
    "all_writers = list(writer_data.keys())\n",
    "train_writers = ['W001', 'W002', 'W003', 'W004', 'W005', 'W006', 'W007', 'W009', 'W010', 'W011', 'W012', 'W015', 'W017', 'W018', 'W019', 'W020', 'W021', 'W022', 'W023', 'W024', 'W025', 'W028', 'W029', 'W030', 'W031', 'W033', 'W034', 'W035', 'W038', 'W039', 'W040', 'W041', 'W042', 'W043', 'W044', 'W045', 'W046', 'W047', 'W049', 'W050', 'W051', 'W052', 'W053', 'W054', 'W058', 'W059', 'W062', 'W063', 'W067', 'W068', 'W069', 'W071', 'W073', 'W074', 'W076', 'W077', 'W081', 'W085', 'W086', 'W087', 'W088', 'W089', 'W090', 'W091', 'W092', 'W093', 'W095', 'W096', 'W097', 'W100']\n",
    "test_writers = ['W013', 'W016', 'W027', 'W032', 'W048', 'W064', 'W065', 'W078', 'W082', 'W083', 'W084', 'W094', 'W098', 'W101']\n",
    "\n",
    "def generate_pairs(writers, mode=\"train\"):\n",
    "    selected_data = {w: writer_data[w] for w in writers if w in writer_data}\n",
    "\n",
    "    # Generate genuine pairs\n",
    "    genuine_pairs = []\n",
    "    for writer, samples in selected_data.items():\n",
    "        if len(samples[\"N\"]) < 2 or len(samples[\"F\"]) < 2:\n",
    "            continue\n",
    "        for _ in range(13):\n",
    "            s1_N, s2_N = random.sample(samples[\"N\"], 2)\n",
    "            s1_F, s2_F = random.sample(samples[\"F\"], 2)\n",
    "            genuine_pairs.append((s1_N, s1_F, s2_N, s2_F, 0))\n",
    "\n",
    "    print(f\"{mode.title()} - Genuine pairs: {len(genuine_pairs)}\")\n",
    "\n",
    "    # Generate impostor pairs\n",
    "    impostor_pairs = []\n",
    "    all_writer_pairs = list(combinations(writers, 2)) \n",
    "    random.shuffle(all_writer_pairs)\n",
    "\n",
    "    writers_covered = set()\n",
    "    for w1, w2 in all_writer_pairs:\n",
    "        if (\n",
    "            selected_data[w1][\"N\"] and selected_data[w1][\"F\"] and\n",
    "            selected_data[w2][\"N\"] and selected_data[w2][\"F\"]\n",
    "        ):\n",
    "            for _ in range(2):\n",
    "                s1_N = random.choice(selected_data[w1][\"N\"])\n",
    "                s1_F = random.choice(selected_data[w1][\"F\"])\n",
    "                s2_N = random.choice(selected_data[w2][\"N\"])\n",
    "                s2_F = random.choice(selected_data[w2][\"F\"])\n",
    "                impostor_pairs.append((s1_N, s1_F, s2_N, s2_F, 1))\n",
    "                # writers_covered.update([w1, w2])\n",
    "        if len(impostor_pairs) > len(genuine_pairs):\n",
    "            break\n",
    "\n",
    "    print(f\"{mode.title()} - Impostor pairs: {len(impostor_pairs)}\")\n",
    "\n",
    "    all_pairs = genuine_pairs + impostor_pairs[:len(genuine_pairs)]\n",
    "    random.shuffle(all_pairs)\n",
    "\n",
    "    df = pd.DataFrame(all_pairs, columns=[\"sample_1_N\", \"sample_1_F\", \"sample_2_N\", \"sample_2_F\", \"label\"])\n",
    "    df.to_csv(f\"uni_{mode}.csv\", index=False)\n",
    "    df.to_parquet(f\"uni_{mode}.parquet\", index=False, compression=\"snappy\")\n",
    "\n",
    "    return genuine_pairs, impostor_pairs\n",
    "\n",
    "generate_pairs(train_writers, mode=\"train\")\n",
    "generate_pairs(test_writers, mode=\"test\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
