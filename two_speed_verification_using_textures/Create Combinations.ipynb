{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6346,
     "status": "ok",
     "timestamp": 1753012071015,
     "user": {
      "displayName": "Hikari Research",
      "userId": "13771564110195447892"
     },
     "user_tz": -330
    },
    "id": "gPCFqzOBAULj",
    "outputId": "7821ebf9-c27a-438f-adfa-f292e725e4d4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import gdown\n",
    "import zipfile\n",
    "import random\n",
    "from itertools import combinations\n",
    "print(tf.__version__)\n",
    "!python --version\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 152223,
     "status": "ok",
     "timestamp": 1753012228038,
     "user": {
      "displayName": "Hikari Research",
      "userId": "13771564110195447892"
     },
     "user_tz": -330
    },
    "id": "io27Gkn1GzrG",
    "outputId": "0ed1ce37-79a1-4aa4-fe88-b13f51a45835"
   },
   "outputs": [],
   "source": [
    "file_id = \"file_id\"\n",
    "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", \"Data_V9_ViT.zip\", quiet=False)\n",
    "with zipfile.ZipFile(\"Data_V9_ViT.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pYKjAn2EJUr"
   },
   "source": [
    "## Parquet Creation Testing - Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1753013107230,
     "user": {
      "displayName": "Hikari Research",
      "userId": "13771564110195447892"
     },
     "user_tz": -330
    },
    "id": "8g8CR7-2EMod",
    "outputId": "9e515bba-6453-40e7-f19d-afbb14e35e2d"
   },
   "outputs": [],
   "source": [
    "BASE_PATH = \"Data_V9_ViT/texture_uni\"\n",
    "random.seed(42)\n",
    "\n",
    "writer_data = {}\n",
    "\n",
    "for writer_id in os.listdir(BASE_PATH):\n",
    "    writer_path = os.path.join(BASE_PATH, writer_id)\n",
    "    if not os.path.isdir(writer_path):\n",
    "        continue\n",
    "\n",
    "    writer_data[writer_id] = {\"N\": [], \"F\": []}\n",
    "\n",
    "    for sample in os.listdir(writer_path):\n",
    "        # sample = e.g., S01_F\n",
    "        sample_parts = sample.split(\"_\")\n",
    "        if len(sample_parts) != 2:\n",
    "            continue\n",
    "        speed = sample_parts[1]  # N or F\n",
    "\n",
    "        sample_path = os.path.join(writer_path, sample)\n",
    "        if not os.path.isdir(sample_path):\n",
    "            continue\n",
    "\n",
    "        for img in os.listdir(sample_path):\n",
    "            if img.endswith(\".pt\"):\n",
    "                img = img.replace(\".pt\", \"\")\n",
    "                rel_path = os.path.join(BASE_PATH, writer_id, sample, img)\n",
    "                writer_data[writer_id][speed].append(rel_path)\n",
    "\n",
    "# Step 2: Split writers\n",
    "all_writers = list(writer_data.keys())\n",
    "train_writers = ['W001', 'W002', 'W003', 'W004', 'W005', 'W006', 'W007', 'W009', 'W010', 'W011', 'W012', 'W015', 'W017', 'W018', 'W019', 'W020', 'W021', 'W022', 'W023', 'W024', 'W025', 'W028', 'W029', 'W030', 'W031', 'W033', 'W034', 'W035', 'W038', 'W039', 'W040', 'W041', 'W042', 'W043', 'W044', 'W045', 'W046', 'W047', 'W049', 'W050', 'W051', 'W052', 'W053', 'W054', 'W058', 'W059', 'W062', 'W063', 'W067', 'W068', 'W069', 'W071', 'W073', 'W074', 'W076', 'W077', 'W081', 'W085', 'W086', 'W087', 'W088', 'W089', 'W090', 'W091', 'W092', 'W093', 'W095', 'W096', 'W097', 'W100']\n",
    "test_writers = ['W013', 'W016', 'W027', 'W032', 'W048', 'W057', 'W064', 'W065', 'W078', 'W082', 'W083', 'W084', 'W094', 'W098', 'W101']\n",
    "val_writers = ['W008', 'W014', 'W026', 'W036', 'W037', 'W055', 'W060', 'W061', 'W066', 'W070', 'W072', 'W075', 'W079', 'W080', 'W099']\n",
    "\n",
    "def generate_pairs(writers, mode=\"train\"):\n",
    "    genuine_pairs = []\n",
    "    impostor_pairs = []\n",
    "\n",
    "    # GENUINE: same writer\n",
    "    for writer in writers:\n",
    "        if len(writer_data[writer][\"N\"]) < 2 or len(writer_data[writer][\"F\"]) < 2:\n",
    "            continue\n",
    "\n",
    "        for _ in range(20):\n",
    "            s1_N, s2_N = random.sample(writer_data[writer][\"N\"], 2)\n",
    "            s1_F, s2_F = random.sample(writer_data[writer][\"F\"], 2)\n",
    "            genuine_pairs.append((s1_N, s1_F, s2_N, s2_F, 0))\n",
    "\n",
    "    # All unique pairs\n",
    "    all_writer_pairs = list(combinations(writers, 2))\n",
    "    random.shuffle(all_writer_pairs)\n",
    "\n",
    "    # Ensure every writer is covered\n",
    "    writers_used = set()\n",
    "    coverage_writer_pairs = []\n",
    "    target_count = len(genuine_pairs)\n",
    "    for w1, w2 in all_writer_pairs:\n",
    "        if w1 not in writers_used or w2 not in writers_used:\n",
    "            coverage_writer_pairs.append((w1, w2))\n",
    "            writers_used.add(w1)\n",
    "            writers_used.add(w2)\n",
    "        if len(writers_used) == len(writers):\n",
    "            break\n",
    "\n",
    "    remaining_pairs = [p for p in all_writer_pairs if p not in coverage_writer_pairs]\n",
    "    random.shuffle(remaining_pairs)\n",
    "\n",
    "    candidate_impostor_pairs = []\n",
    "\n",
    "    # Generate from coverage pairs\n",
    "    while len(candidate_impostor_pairs) < target_count:\n",
    "        for w1, w2 in all_writer_pairs:\n",
    "            if (\n",
    "                writer_data[w1][\"N\"]\n",
    "                and writer_data[w1][\"F\"]\n",
    "                and writer_data[w2][\"N\"]\n",
    "                and writer_data[w2][\"F\"]\n",
    "            ):\n",
    "                s1_N = random.choice(writer_data[w1][\"N\"])\n",
    "                s1_F = random.choice(writer_data[w1][\"F\"])\n",
    "                s2_N = random.choice(writer_data[w2][\"N\"])\n",
    "                s2_F = random.choice(writer_data[w2][\"F\"])\n",
    "                candidate_impostor_pairs.append((s1_N, s1_F, s2_N, s2_F, 1))\n",
    "                if len(candidate_impostor_pairs) >= target_count:\n",
    "                    break\n",
    "\n",
    "    impostor_pairs = candidate_impostor_pairs[:target_count]\n",
    "\n",
    "\n",
    "    print(f\"{mode.title()} Set â€” Genuine: {len(genuine_pairs)}, Impostor: {len(impostor_pairs)}\")\n",
    "\n",
    "    all_pairs = genuine_pairs + impostor_pairs\n",
    "    random.shuffle(all_pairs)\n",
    "\n",
    "    df_all = pd.DataFrame(all_pairs, columns=[\"sample_1_N\", \"sample_1_F\", \"sample_2_N\", \"sample_2_F\", \"label\"])\n",
    "    df_all.to_csv(f\"uni_{mode}.csv\", index=False)\n",
    "    df_all.to_parquet(\n",
    "        f\"uni_{mode}.parquet\",\n",
    "        index=False,\n",
    "        compression=\"snappy\"\n",
    "    )\n",
    "    return genuine_pairs, impostor_pairs\n",
    "\n",
    "generate_pairs(train_writers, mode=\"train\")\n",
    "generate_pairs(test_writers, mode=\"test\")\n",
    "generate_pairs(test_writers, mode=\"test\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eOxUHLQRW-mv"
   ],
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
